{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12. 텍스트 다루기",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMc54l9RwTs1mTvfqBFW54X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Honggu12/Machine-Learning/blob/main/12_%ED%85%8D%EC%8A%A4%ED%8A%B8_%EB%8B%A4%EB%A3%A8%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJhjCAt_-zsL"
      },
      "source": [
        "# Tokenization(토큰화) 이론\n",
        "텍스트 (문장, 문단, 문서)에서 어디 까지가 문장이고, 무엇이 단어인지를 알려주는 것을 의미한다.\n",
        "\n",
        " - 문장 토큰화\n",
        " - 단어 토큰화\n",
        " - subword 토큰화 (영어의 -er,-est / 한국어의 ~화 등)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn4_VY3u_JvV"
      },
      "source": [
        "sample_text = \"I never thought through love we'd be. Making one as lovely as she. But isn't she lovely made from love\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBho6GctBHGb"
      },
      "source": [
        "문장 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXJZqxiVBk-i",
        "outputId": "d31e68e4-d9e1-45ae-e011-6052950134f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenized_sentence = sample_text.split(\". \")\n",
        "tokenized_sentence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"I never thought through love we'd be\",\n",
              " 'Making one as lovely as she',\n",
              " \"But isn't she lovely made from love\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7R9f3-TBrAt",
        "outputId": "6a690a20-067d-4b7b-adb4-2433c9f7b044",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenized_word = sample_text.split()\n",
        "tokenized_word"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'never',\n",
              " 'thought',\n",
              " 'through',\n",
              " 'love',\n",
              " \"we'd\",\n",
              " 'be.',\n",
              " 'Making',\n",
              " 'one',\n",
              " 'as',\n",
              " 'lovely',\n",
              " 'as',\n",
              " 'she.',\n",
              " 'But',\n",
              " \"isn't\",\n",
              " 'she',\n",
              " 'lovely',\n",
              " 'made',\n",
              " 'from',\n",
              " 'love']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPDl3HnBDA8n"
      },
      "source": [
        "## 띄어쓰기로 영어 문장 내 단어 구분할 때의 문제점\n",
        " - We're Genius!!\n",
        " - We are Genius!!\n",
        " - We are Genius\n",
        "\n",
        " 위 세 문장을 각각 토큰화 하면 사람은 세 문장이 전부 다 똑같은 의미라는 것을 알 수 있으나, 기계는 세 문장이 다르다고 판단 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKE8HPxYDOmV"
      },
      "source": [
        "## 해결 방법 첫 번째 - 특수 문자 제거를 이용해 단어 구분\n",
        " - [We, re, Genius]\n",
        " - [We, are, Genius]\n",
        " - [We, are, Genius]\n",
        "\n",
        " 특수문자가 중요한 의미를 가지는 경우에도 특수문자를 삭제 하는게 맞을까?\n",
        "\n",
        " - $12.45 -> [12, 45]\n",
        " - Mr.So -> [Mr, So]\n",
        " - Mrs. Kim -> [Mrs, Kim]\n",
        " - 192.168.0.1 -> [192, 168, 0, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvBXGAZqDuKR"
      },
      "source": [
        "## 영어 단어 토크나이저 활용하기\n",
        "- TreebankWorkdTokenizer 패키지가 있습니다!\n",
        "  - 영어 표준 토큰화 규격을 따라갑니다.\n",
        "  - Penn Treebank Tokenization 규칙\n",
        "\n",
        "- TreebankWordTokenizer의 규칙\n",
        "  - 하이픈으로 구성된 단어는 하나로 유지\n",
        "  - dosen't 같이 어퍼스트로피로 '접어'가 함께하는 단어는 따로 분리해 준다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rtVjXd5FNMS"
      },
      "source": [
        "## 한국어의 토큰화가 어려운 이유 \n",
        "\n",
        "1. 한국어는 교착어이다.\n",
        "2. 한국어는 띄어쓰기가 잘 지켜지지 않는다.\n",
        "3. 한국어는 주어생략이 가능하고, 어순도 중요하지 않다.\n",
        "4. 한자어라는 특성상 하나의 음절도 다른 의미를 가질 수 있다.\n",
        "\n",
        "### 교착어\n",
        "실질적인 의미를 가지는 어간에 조사나 어미와 같은 문법 형태소가 결합하여 문법적인 기능(각각이 다른 의미를 갖는)이 부여되는 언어\n",
        "\n",
        "### 띄어쓰기 문제\n",
        "한국어는띄어쓰기를하지않아도일단읽을수는있다.\n",
        "영어는 힘들다.\n",
        "\n",
        "'py-hanspell' 패키지 또는 'ko-spacing' 패키지를 이용해 문법이나 띄어쓰기 교정을 할 수 있다.\n",
        "\n",
        "### 주어 생략 및 어순 문제\n",
        "1. 나는 운동을 했어. 체육관에서.\n",
        "2. 나는 체육관에서 운동을 했어.\n",
        "3. 체육관에서 운동했어.\n",
        "4. 나는 운동을 체육관에서 했어.\n",
        "\n",
        "### 한자어 특성 때문에 하나의 음적이 다른 의미를 갖는 것\n",
        "1. 배 라는 단어는 사람의 배, 타는 배, 먹는 배\n",
        "2. 한국도 한은 한국 한, 국은 나라 국 이지만 각자 다른 의미로 해석될 수도 있다.(된장국, 미역국 등...)\n",
        "\n",
        "한국어의 형태소 분석을 쉽게 하기 위해 konlpy 패키지 활용\n",
        "!pip install konlpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1LjkuP6HrYE"
      },
      "source": [
        "### Mecab\n",
        "실무에서 가장 많이 사용되고 있는 형태소 분석기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5sLM9n5NKuT",
        "outputId": "7f205512-88cf-4ebc-c489-50f04152044e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "'''\n",
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "%cd Mecab-ko-for-Google-Colab\n",
        "!bash install_mecab-ko_on_colab190912.sh\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\\n%cd Mecab-ko-for-Google-Colab\\n!bash install_mecab-ko_on_colab190912.sh\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij0yhbDlNTkg"
      },
      "source": [
        "#Twitter, 꼬꼬마, 코모란, 한나눔\n",
        "속도가 Mecab이나 khali보다 많이 느리지만 각각의 장단점이 있음."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cqj5Ri5DNfC3"
      },
      "source": [
        "## Cleaning(정제)과 Normalization(정규화)\n",
        "\n",
        "### 정제란?\n",
        " - 불필요한 데이터를 제거하는 것\n",
        " - 텍스트 중간중간 껴있는 숫자나 특수기호를 제거하는 것\n",
        " - 한국어의 경우 은, 는, 이, 가 등의 불용어(stopwords)를 제거할 때 사용함\n",
        " - 영어의 경우는 at, is, am, the 등을 제거한다.\n",
        " - 텍스트의 인코딩 문제 해결\n",
        " - 길이가 짧은 단어들 제거\n",
        " - 등장 빈도가 적은 단어 제거\n",
        "\n",
        " ### 정규화란?\n",
        "  - 문장의 복잡도를 줄여주는 과정\n",
        "  - 같은 의미를 가지고 있는 여러 단어를 하나로 통합하는 작업\n",
        "  - 영어의 경우 lemmatization\n",
        "  - am, are, were, was -> be\n",
        "  - has, had -> have\n",
        "  - 10, 12, 100 -> num\n",
        "  - ㅋ, ㅋㅋㅋㅋ, ㅋㅋㅋㅋㅋ -> ㅋㅋ\n",
        "  - 대소문자 통합 등\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nchuSDWuObHi"
      },
      "source": [
        "Tokenization 실습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUdLG56vPoxr",
        "outputId": "711cf15c-4414-4c0f-b50e-dcc2e14b4507",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt') # 영어 토크나이저 활용하기\n",
        "sentence = \"Ain't nothin' sweeter, you want this sugar, don't ya?\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNr5cATDP1Rj"
      },
      "source": [
        "## 기본 토크나이저"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPodfPJbP9-b",
        "outputId": "87a827b5-a744-4f55-f043-96e44c7c30e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "print(word_tokenize(sentence))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Ai', \"n't\", 'nothin', \"'\", 'sweeter', ',', 'you', 'want', 'this', 'sugar', ',', 'do', \"n't\", 'ya', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNzQLQOeQKtN"
      },
      "source": [
        "WordPunkTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G926gLMnQMw0",
        "outputId": "c98bbdc6-1380-4ff2-a8db-8e6043c0b600",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "print(WordPunctTokenizer().tokenize(sentence))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Ain', \"'\", 't', 'nothin', \"'\", 'sweeter', ',', 'you', 'want', 'this', 'sugar', ',', 'don', \"'\", 't', 'ya', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4AposklQSOM"
      },
      "source": [
        "## TreebankWordTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0mHeT4LQdmF",
        "outputId": "0b0f0682-831c-43ad-f6c3-3aad26506006",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "\n",
        "print(tokenizer.tokenize(sentence))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Ai', \"n't\", 'nothin', \"'\", 'sweeter', ',', 'you', 'want', 'this', 'sugar', ',', 'do', \"n't\", 'ya', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyOtAfJ3Q1rk",
        "outputId": "11ab1923-d675-4192-d65d-6e08726b85be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sample_text = \"I'm Iron-man\"\n",
        "print(tokenizer.tokenize(sample_text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', \"'m\", 'Iron-man']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRettvG3RLQp"
      },
      "source": [
        "## 한글 토크나이저\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syXI45x-RNf4",
        "outputId": "866493b3-221c-4e2e-c2e4-a08a4f761355",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 1.5MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/96/1030895dea70855a2e1078e3fe0d6a63dcb7c212309e07dc9ee39d33af54/JPype1-1.1.2-cp36-cp36m-manylinux2010_x86_64.whl (450kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 36.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 8.7MB/s \n",
            "\u001b[?25hCollecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/7c/99d51f80f3b77b107ebae2634108717362c059a41384a1810d13e2429a81/tweepy-3.9.0-py2.py3-none-any.whl\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, beautifulsoup4, tweepy, colorama, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "Successfully installed JPype1-1.1.2 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 tweepy-3.9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wy15uroRQLJ",
        "outputId": "8376b70b-f6d1-4626-f24b-b53d63b48f9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "%cd Mecab-ko-for-Google-Colab\n",
        "!bash install_mecab-ko_on_colab190912.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "make\n",
            "make check\n",
            "make install\n",
            "ldconfig\n",
            "Done\n",
            "Change Directory to /content\n",
            "Downloading mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "--2020-11-10 08:34:50--  https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22c0:3470, 2406:da00:ff00::22cd:e0db, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=XMvvaO0nKzQ%2BXC2ngydjKSZgelE%3D&Expires=1604998188&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None [following]\n",
            "--2020-11-10 08:34:50--  https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=XMvvaO0nKzQ%2BXC2ngydjKSZgelE%3D&Expires=1604998188&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.217.49.212\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.217.49.212|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49775061 (47M) [application/x-tar]\n",
            "Saving to: ‘mecab-ko-dic-2.1.1-20180720.tar.gz.1’\n",
            "\n",
            "mecab-ko-dic-2.1.1- 100%[===================>]  47.47M  70.3MB/s    in 0.7s    \n",
            "\n",
            "2020-11-10 08:34:51 (70.3 MB/s) - ‘mecab-ko-dic-2.1.1-20180720.tar.gz.1’ saved [49775061/49775061]\n",
            "\n",
            "Done\n",
            "Unpacking  mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-ko-dic-2.1.1-20180720\n",
            "Done\n",
            "installing........\n",
            "configure\n",
            "make\n",
            "make install\n",
            "apt-get update\n",
            "apt-get upgrade\n",
            "apt install curl\n",
            "apt install git\n",
            "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
            "Done\n",
            "Successfully Installed\n",
            "Now you can use Mecab\n",
            "from konlpy.tag import Mecab\n",
            "mecab = Mecab()\n",
            "사용자 사전 추가 방법 : https://bit.ly/3k0ZH53\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0nBxXErSr5k"
      },
      "source": [
        "from konlpy.tag import Hannanum\n",
        "from konlpy.tag import Kkma\n",
        "from konlpy.tag import Komoran\n",
        "from konlpy.tag import Okt\n",
        "from konlpy.tag import Mecab\n",
        "\n",
        "hannanum = Hannanum()\n",
        "kkma = Kkma()\n",
        "komoran = Komoran()\n",
        "okt = Okt()\n",
        "mecab = Mecab()\n",
        "\n",
        "sentence = \"좋으니 그 사람 솔직히 견디기 버거워\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0G2irNiTCxP"
      },
      "source": [
        "## 트위터(Okt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C80GlHPSThlo",
        "outputId": "b6be4c3e-9600-40fd-fac8-488aa5f0af72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(okt.nouns(sentence)) # 명사만 추출\n",
        "print(okt.morphs(sentence)) # 각 형태소 별로 토큰화\n",
        "print(okt.pos(sentence)) # 각 형태소 토큰 및 형태소 종류를 튜플로 표시\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['그', '사람']\n",
            "['좋으니', '그', '사람', '솔직히', '견디기', '버거워']\n",
            "[('좋으니', 'Adjective'), ('그', 'Noun'), ('사람', 'Noun'), ('솔직히', 'Adjective'), ('견디기', 'Verb'), ('버거워', 'Adjective')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifUvl-SvTuUQ"
      },
      "source": [
        "## 꼬꼬마(Kkma)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvcZYr9pT01N",
        "outputId": "4ae90b6a-1116-4b6b-ecf2-2345e0339abc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(kkma.nouns(sentence)) # 명사만 추출\n",
        "print(kkma.morphs(sentence)) # 각 형태소 별로 토큰화\n",
        "print(kkma.pos(sentence)) # 각 형태소 토큰 및 형태소 종류를 튜플로 표시"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['사람']\n",
            "['좋', '으니', '그', '사람', '솔직히', '견디', '기', '버겁', '어']\n",
            "[('좋', 'VA'), ('으니', 'ECD'), ('그', 'MDT'), ('사람', 'NNG'), ('솔직히', 'MAG'), ('견디', 'VV'), ('기', 'ETN'), ('버겁', 'VA'), ('어', 'ECS')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fy6RXiFT3-z"
      },
      "source": [
        "## 코모란(Komoran)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9SmQXAoUFNl",
        "outputId": "92950037-321f-4dd9-b51e-b281b3bf3604",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(komoran.nouns(sentence)) # 명사만 추출\n",
        "print(komoran.morphs(sentence)) # 각 형태소 별로 토큰화\n",
        "print(komoran.pos(sentence)) # 각 형태소 토큰 및 형태소 종류를 튜플로 표시"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['사람']\n",
            "['좋', '으니', '그', '사람', '솔직히', '견디', '기', '버거워']\n",
            "[('좋', 'VA'), ('으니', 'EC'), ('그', 'MM'), ('사람', 'NNG'), ('솔직히', 'MAG'), ('견디', 'VV'), ('기', 'ETN'), ('버거워', 'NA')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUXA8pOWUJSp"
      },
      "source": [
        "## 한나눔"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6rsLRxLULwz",
        "outputId": "3d01fd3b-9c18-4ee1-b84e-e74d4c741255",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(hannanum.nouns(sentence)) # 명사만 추출\n",
        "print(hannanum.morphs(sentence)) # 각 형태소 별로 토큰화\n",
        "print(hannanum.pos(sentence)) # 각 형태소 토큰 및 형태소 종류를 튜플로 표시"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['사람', '버거워']\n",
            "['좋', '으니', '그', '사람', '솔직히', '견디', '기', '버거워']\n",
            "[('좋', 'P'), ('으니', 'E'), ('그', 'M'), ('사람', 'N'), ('솔직히', 'M'), ('견디', 'P'), ('기', 'E'), ('버거워', 'N')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ1DMBhfUbrH"
      },
      "source": [
        "##메캅(Mecab)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBpI8Zx-UfLJ",
        "outputId": "fa8a1939-d8ca-48d9-d877-09a9cdbdfcaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(mecab.nouns(sentence)) # 명사만 추출\n",
        "print(mecab.morphs(sentence)) # 각 형태소 별로 토큰화\n",
        "print(mecab.pos(sentence)) # 각 형태소 토큰 및 형태소 종류를 튜플로 표시"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['사람']\n",
            "['좋', '으니', '그', '사람', '솔직히', '견디', '기', '버거워']\n",
            "[('좋', 'VA'), ('으니', 'EC'), ('그', 'MM'), ('사람', 'NNG'), ('솔직히', 'MAG'), ('견디', 'VV'), ('기', 'ETN'), ('버거워', 'VA+EC')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6RQT38DUh3z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}